# -*- coding: utf-8 -*-
"""Car Prices Prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T-0tbHkD35LTUphA3dhfedl2XqhXkuY9

**Importing Libraries**
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

"""**Reading File**"""

data = pd.read_csv("//content//car_price_prediction.csv")

"""**Data Exploration & Cleaning**"""

data.head()

data.shape

data.info()

data.describe()

data.duplicated().sum()

data.drop_duplicates(inplace=True)
data.duplicated().sum()

data.isnull().sum()

"""**Data Analysis**"""

for column in data.columns:        #counting all unique values in all columns
  print(column, ":", data[column].nunique())

data.hist(bins=10,figsize=(15,10))
plt.show()

top_10_cars = data["Manufacturer"].value_counts().sort_values(ascending=False)[:10] #top 10 cars we have
top_10_cars

car_brands = ['HYUNDAI', 'TOYOTA', 'MERCEDES-BENZ', 'FORD', 'CHEVROLET', 'BMW', 'HONDA', 'LEXUS', 'NISSAN', 'VOLKSWAGEN']
car_counts = [3729, 3606, 2043, 1088, 1047, 1036, 960, 927, 645, 571]

# Set the style
sns.set_style("dark")


# Create the bar plot
sns.barplot(x=car_counts, y=car_brands)

# Add labels and title
plt.xlabel('Counts')
plt.ylabel('Car Brands')
plt.title('Top 10 Cars')

# Display the plot
plt.show()

top_10_mean_prices = [data[data["Manufacturer"] == i]["Price"].mean() for i in list(top_10_cars.index)] #calculating the average price for each manufacture
top_10_mean_prices

# Create the bar plot
sns.barplot(x=top_10_cars.index, y=top_10_mean_prices)

# Set the font size on the x-axis
plt.xticks(fontsize=5.5)

# Display the plot
plt.show()

cor=data.corr()  #meausring correlation
cor

sns.heatmap(cor,annot=True,linewidth=0.5,fmt=".3f")
plt.show()

data_object = data.select_dtypes(include="object")
data_object.info()

for col in data_object :
  plt.figure(figsize=(15,5))
  top10= data[col].value_counts()[:10]
  colors = ["b","r","g","y","orange"]
  top10.plot(kind="bar",color=colors)
  plt.title( "Top 10 :" + col)
  plt.show()

"""**Data Processing**"""

data=data.drop(["ID","Doors"],axis=1) #deleting unneccesery features

import datetime   #Editing dates to be fit with today date
date= datetime.datetime.now()

data["Age"]=date.year - data["Prod. year"]
data=data.drop(["Prod. year"],axis=1)

data.head()

data["Levy"] = data["Levy"].replace("-", "0")  #replacing (-) in column levy with (0)
data["Levy"] = data["Levy"].astype(int) #converting to int

data["Mileage"]=data["Mileage"].str.replace("km","")   #deleting word (km) from Mileage column
data.head()

data["Engine volume"]= data["Engine volume"].str.replace("Turbo","") #deleting word (turpo) from Mileage column Engine volume
data["Engine volume"] = data["Engine volume"].astype(float) #converting to float

data.info()

"""**Detecting Outliers**"""

data_numeric = data.select_dtypes(exclude="object")
for col in data_numeric:
  q1=data[col].quantile(0.25)
  q3=data[col].quantile(0.75)
  iqr = q3-q1
  low = q1-1.5*iqr
  high = q3+1.5*iqr
  outlier = ((data_numeric[col]>high) | (data_numeric[col]<low)).sum()
  total = data_numeric[col].shape[0]
  print(f"Total outliers in {col} are : {outlier}---{round(100*(outlier)/total,2)}")
  if outlier > 0 :
    data=data.loc[(data[col]<=high) & ((data[col]>=low)) ]

"""Data Transforming"""

dobject = data.select_dtypes(include= "object")
dnumeic = data.select_dtypes(exclude= "object")

from sklearn.preprocessing import LabelEncoder
lab= LabelEncoder()

for i in range(0,dobject.shape[1]):
  dobject.iloc[:,i]=lab.fit_transform(dobject.iloc[:,i])

data=pd.concat([dobject,dnumeic],axis=1)
data.info()

"""**Creating Model**"""

x= data.drop("Price",axis=1)
y= data["Price"].values.reshape(-1,1)

from sklearn.model_selection import train_test_split
np.random.seed(0)

x_train,x_test,y_train,y_test= train_test_split(x,y,train_size=0.7,random_state=44)

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.svm import SVR
from sklearn.metrics import r2_score , mean_squared_error , mean_absolute_percentage_error

algorithms = ["LinearRegression","DecisionTreeRegressor","RandomForestRegressor","GradientBoostingRegressor","XGBRegressor","SVR"]
R2=[]
RMSE=[]
SCORE=[]

def testing(model):
  model.fit(x_train,y_train)
  pred= model.predict(x_test)
  r2=r2_score(y_test,pred)
  rmse=np.sqrt(mean_squared_error(y_test,pred))
  score = model.score(x_test,y_test)
  R2.append(r2)
  RMSE.append(rmse)
  SCORE.append(score)
  print(f"Model score is {score}")

model1= LinearRegression()
model2= DecisionTreeRegressor()
model3= RandomForestRegressor()
model4= GradientBoostingRegressor()
model5= XGBRegressor()
model6= SVR()

testing(model1)
testing(model2)
testing(model3)
testing(model4)
testing(model5)
testing(model6)

Results= pd.DataFrame({"algorithms": algorithms ,"score": SCORE ,"r2": R2,"rmse": RMSE})
Results

